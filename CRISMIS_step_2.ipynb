{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CRISMIS2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNEpERFZnk1-"
      },
      "source": [
        "# This notebook is majorly for down-loading and viewing the images and creating the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJb7Y9YrJ8XJ"
      },
      "source": [
        "#importing important libraries\n",
        "import requests\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS_jLxbQKE9l"
      },
      "source": [
        "# checks the validity of the url\n",
        "# url should have netloc(domain name) and scheme(protocol)\n",
        "def is_valid(url):\n",
        "    parsed = urlparse(url)\n",
        "    return bool(parsed.netloc) and bool(parsed.scheme)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnpU3hhaZe4k"
      },
      "source": [
        "# makes the url of a particular year and day\n",
        "def get_url_from_day_year (year,day):\n",
        "  url = \"https://pdsimage2.wr.usgs.gov/archive/mess-e_v_h-mdis-2-edr-rawdata-v1.0/MSGRMDS_1001/DATA/\" + str(year) + '_' + str(day) + '/'\n",
        "  return url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQJoqbx1TuNA"
      },
      "source": [
        "# return a list of all the IMG files which are their on a url of particular year and day.\n",
        "# here input is a particular url which is of that particular year and day.\n",
        "def get_all_images(url):\n",
        "# html parser is used \n",
        "    soup = bs(requests.get(url).content, \"html.parser\")\n",
        "    urls = []\n",
        "    \n",
        "# tqdm is used to see the progress of the loop\n",
        "    for img in tqdm(soup.find_all(\"a\"), \"Extracting images\"):\n",
        "        img_url = img.attrs.get(\"href\")\n",
        "        IMG_checker = \"IMG\"\n",
        "\n",
        "# IMG_checker is used to filter href link that contain .IMG format data. \n",
        "# make the URL absolute by joining domain with the URL that is just extracted\n",
        "# the url is checked for it's validity\n",
        "        if IMG_checker in img_url:\n",
        "          img_url = urljoin(url, img_url)\n",
        "          if is_valid(img_url):\n",
        "            urls.append(img_url)           \n",
        "    return urls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLtJgsDIUVyL"
      },
      "source": [
        "# return a list of urls of all .IMG format image data files of a particular year and day.\n",
        "# here input is year,day\n",
        "\n",
        "def images_list (year,day):\n",
        "  url = get_url_from_day_year (year,day)\n",
        "  images_url_list = get_all_images(url)\n",
        "  return images_url_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNdMUv8rUlYH"
      },
      "source": [
        "# download function download an image and save it at the pathname\n",
        "# url of a particular image file should be given\n",
        "def download(url, pathname):\n",
        "\n",
        "    # if path doesn't exist, make that path dir\n",
        "    if not os.path.isdir(pathname):\n",
        "        os.makedirs(pathname)\n",
        "    # download the body of response by chunk, not immediately\n",
        "    response = requests.get(url, stream=True)\n",
        "    # get the total file size\n",
        "    file_size = int(response.headers.get(\"Content-Length\", 0))\n",
        "    # get the file name\n",
        "    filename = os.path.join(pathname, url.split(\"/\")[-1])\n",
        "    # progress bar, changing the unit to bytes instead of iteration (default by tqdm)\n",
        "    progress = tqdm(response.iter_content(1024), f\"Downloading {filename}\", total=file_size, unit=\"B\", unit_scale=True, unit_divisor=1024)\n",
        "    with open(filename, \"wb\") as f:\n",
        "        for data in progress:\n",
        "            # write data read to the file\n",
        "            f.write(data)\n",
        "            # update the progress bar manually\n",
        "            progress.update(len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MthQtBNG-LKG"
      },
      "source": [
        "# this function download all the image data files of a particular year and day \n",
        "# and stores all of them at pathname \n",
        "def retrive_all_images (year,day,pathname):\n",
        "  images_url_list = images_list(year,day)\n",
        "  for url in images_url_list:\n",
        "    download(url,pathname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEedyBfy_tb9"
      },
      "source": [
        "# this function gives url of a particular image data file of a particular year and day\n",
        "# particular image data file should be given with .IMG format.\n",
        "def get_specific_url(year,day,file_name):\n",
        "  images_url_list = images_list(year,day)\n",
        "  for images_url in images_url_list:\n",
        "    if file_name in images_url:\n",
        "      return images_url\n",
        "  return (\" This file does not exist.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Stur3xIColzy"
      },
      "source": [
        "# removes the .IMG from the file name \n",
        "def file_num (string):\n",
        "  final_str = \"\"\n",
        "  for character in string:\n",
        "    if (character == \".\"):\n",
        "      return final_str\n",
        "    final_str = final_str + character    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvsegaO0ouTl"
      },
      "source": [
        "# shape_function returns a type of shape which is needed for reshaping the array, this has been done for only 4 cases,\n",
        "def shape_function (data):\n",
        "  if len(data) == 134656 : \n",
        "    return (526,256)\n",
        "  if len(data) == 527872 :\n",
        "    return (1031,512)\n",
        "  if len(data) == 1052672:\n",
        "    return (1028,1024)\n",
        "  if len(data) == 528384 :\n",
        "    return (688,768)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utlHgopWa6RC"
      },
      "source": [
        "# this function is almost similar to a previous function open_specific_image the only change is this function directly takes the input of the image data file url.\n",
        "# the image file can again be saved by uncommenting output_filename and plt.save()\n",
        "def specific_image(file_path,url):\n",
        "  download(url,file_path)\n",
        "\n",
        "  # Parameters.\n",
        "  input_filename = file_path + '/' + url.split(\"/\")[-1]\n",
        "  print(input_filename)\n",
        "  dtype = np.dtype('>u2') # big-endian unsigned integer (16bit)\n",
        "  #output_filename = jpeg_path + '/' + str(file_num(url.split(\"/\")[-1])) + '.JPG'\n",
        "  #print(output_filename)\n",
        "\n",
        "  # Reading.\n",
        "  fid = open(input_filename, 'rb')\n",
        "  data = np.fromfile(fid, dtype)\n",
        "  shape = shape_function(data) # matrix size\n",
        "  image = data.reshape(shape)\n",
        "\n",
        "  # Display.\n",
        "  plt.figure(figsize=(8,5), dpi= 250)\n",
        "  plt.imshow(image, cmap = \"gray\", )\n",
        "  #plt.savefig(output_filename)\n",
        "  plt.show()\n",
        "  return data,image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN7QljTxdmD7"
      },
      "source": [
        "# this function return the maximum value in the image data file, it's location and the mean of all the data in image data file\n",
        "def maximum_index (image) : \n",
        "  list12 = []\n",
        "  for index in image:\n",
        "    list12.append(index.max())\n",
        "  maximum  = np.max(list12)\n",
        "  list11 = []\n",
        "  for index in range(len(image)):\n",
        "    if (maximum in image[index]):\n",
        "      for number in range(len(image[index])):\n",
        "        if maximum == image[index][number]:\n",
        "          list11.append((index,number))\n",
        "  print(maximum,list11,np.mean(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKFcd6xX64-6"
      },
      "source": [
        "# list of all urls of image data is created for year 2011 and day 155\n",
        "images_2011_155 = images_list(2011,155)\n",
        "print(len(images_2011_155))\n",
        "\n",
        "# the list of url are looped and passed to specific_image function and plotted\n",
        "# and the image_data is also passed to maximum_index function \n",
        "for urls in images_2011_155:\n",
        "  data,image = specific_image(\"/content/images\",urls)\n",
        "  maximum_index(image)\n",
        "\n",
        "# the list_of_url is exported in .csv format \n",
        "url_dataframe = pd.DataFrame(images_2011_155)\n",
        "url_dataframe.to_csv('/content/url.csv',index = False,header= False)\n",
        "\n",
        "# the same is done for year 2011, days 156,157,158"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAJSHdAYsUSa"
      },
      "source": [
        "# the four days images data are stored in four different folders images, images1, images2, images3\n",
        "# and exported in 4 different .csv files named url,url1,url2,url3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RxR5YhnA-nP"
      },
      "source": [
        "images_2011_156 = images_list(2011,156)\n",
        "len(images_2011_156)\n",
        "for urls in images_2011_156:\n",
        "  data,image = specific_image(\"/content/images1\",urls)\n",
        "  maximum_index(image)\n",
        "url_dataframe = pd.DataFrame(images_2011_156)\n",
        "url_dataframe.to_csv('/content/url1.csv',index = False,header= False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmulkKUcTPPP"
      },
      "source": [
        "images_2011_157 = images_list(2011,157)\n",
        "print(len(images_2011_157))\n",
        "for urls in images_2011_157:\n",
        "  data,image = specific_image(\"/content/images2\",urls)\n",
        "  maximum_index(image)\n",
        "url_dataframe = pd.DataFrame(images_2011_157)\n",
        "url_dataframe.to_csv('/content/url2.csv',index = False,header= False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkNbhihstHUr"
      },
      "source": [
        "images_2011_158 = images_list(2011,158)\n",
        "for urls in images_2011_158:\n",
        "  data,image = specific_image(\"/content/images3\",urls)\n",
        "  maximum_index(image)\n",
        "\n",
        "url_dataframe = pd.DataFrame(images_2011_158)\n",
        "url_dataframe.to_csv('/content/url3.csv',index = False,header= False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxJe2vPIcwdk"
      },
      "source": [
        "# now based on data above all the data from 4th to 7th of june 2011 have been classified with manually checking of the images and with\n",
        "# knowing which at which point highest image data is there and where is it\n",
        "# these classification are saved in the previously exported files\n",
        "# and have the same name same name of exported ones. url,url1,url2,url3 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0rZu1hm2wke"
      },
      "source": [
        "#the whole train and test data set has been made through data in these four .csv files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4poyFGwJto0s"
      },
      "source": [
        "### **COSMIC RAY ARTIFACT - 1**\n",
        "### **NOT AN ARTIFACT -  0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0KKwqlluc_Z"
      },
      "source": [
        "### The .csv files with classfication can be founded here : https://github.com/VIDIT-OSTWAL/CRISMIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXoD7fROhqJX"
      },
      "source": [
        "csv = pd.read_csv(\"/content/url.csv\",header = None,index_col=  False,names = ['File Name','Classification'])\n",
        "csv1 = pd.read_csv(\"/content/url1.csv\",header = None,index_col=  False,names = ['File Name','Classification'])\n",
        "csv2= pd.read_csv(\"/content/url2.csv\",header = None,index_col=  False,names = ['File Name','Classification'])\n",
        "csv3 = pd.read_csv(\"/content/url3.csv\",header = None,index_col=  False,names = ['File Name','Classification'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}